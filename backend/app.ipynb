{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "LCjoSokg70O8"
      },
      "outputs": [],
      "source": [
        "!fuser -k 8000/tcp"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9IaDAtPq11P_",
        "outputId": "e20e803a-ff84-4515-f05c-afe2945b4521"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.47.1)\n",
            "Collecting fastapi\n",
            "  Downloading fastapi-0.115.7-py3-none-any.whl.metadata (27 kB)\n",
            "Collecting uvicorn\n",
            "  Downloading uvicorn-0.34.0-py3-none-any.whl.metadata (6.5 kB)\n",
            "Collecting python-magic\n",
            "  Downloading python_magic-0.4.27-py2.py3-none-any.whl.metadata (5.8 kB)\n",
            "Collecting PyPDF2\n",
            "  Downloading pypdf2-3.0.1-py3-none-any.whl.metadata (6.8 kB)\n",
            "Requirement already satisfied: sentence-transformers in /usr/local/lib/python3.11/dist-packages (3.3.1)\n",
            "Collecting pinecone-client\n",
            "  Downloading pinecone_client-5.0.1-py3-none-any.whl.metadata (19 kB)\n",
            "Collecting python-dotenv\n",
            "  Downloading python_dotenv-1.0.1-py3-none-any.whl.metadata (23 kB)\n",
            "Collecting groq\n",
            "  Downloading groq-0.15.0-py3-none-any.whl.metadata (14 kB)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers) (3.16.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.24.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.27.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (1.26.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.21.0)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.5.2)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers) (4.67.1)\n",
            "Collecting starlette<0.46.0,>=0.40.0 (from fastapi)\n",
            "  Downloading starlette-0.45.2-py3-none-any.whl.metadata (6.3 kB)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,!=2.0.0,!=2.0.1,!=2.1.0,<3.0.0,>=1.7.4 in /usr/local/lib/python3.11/dist-packages (from fastapi) (2.10.5)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.11/dist-packages (from fastapi) (4.12.2)\n",
            "Requirement already satisfied: click>=7.0 in /usr/local/lib/python3.11/dist-packages (from uvicorn) (8.1.8)\n",
            "Requirement already satisfied: h11>=0.8 in /usr/local/lib/python3.11/dist-packages (from uvicorn) (0.14.0)\n",
            "Requirement already satisfied: torch>=1.11.0 in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (2.5.1+cu121)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (1.6.0)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (1.13.1)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (11.1.0)\n",
            "Requirement already satisfied: certifi>=2019.11.17 in /usr/local/lib/python3.11/dist-packages (from pinecone-client) (2024.12.14)\n",
            "Collecting pinecone-plugin-inference<2.0.0,>=1.0.3 (from pinecone-client)\n",
            "  Downloading pinecone_plugin_inference-1.1.0-py3-none-any.whl.metadata (2.2 kB)\n",
            "Collecting pinecone-plugin-interface<0.0.8,>=0.0.7 (from pinecone-client)\n",
            "  Downloading pinecone_plugin_interface-0.0.7-py3-none-any.whl.metadata (1.2 kB)\n",
            "Requirement already satisfied: urllib3>=1.26.0 in /usr/local/lib/python3.11/dist-packages (from pinecone-client) (2.3.0)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.11/dist-packages (from groq) (3.7.1)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.11/dist-packages (from groq) (1.9.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from groq) (0.28.1)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.11/dist-packages (from groq) (1.3.1)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.11/dist-packages (from anyio<5,>=3.5.0->groq) (3.10)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->groq) (1.0.7)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.24.0->transformers) (2024.10.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,!=2.0.0,!=2.0.1,!=2.1.0,<3.0.0,>=1.7.4->fastapi) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.27.2 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,!=2.0.0,!=2.0.1,!=2.1.0,<3.0.0,>=1.7.4->fastapi) (2.27.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (3.1.5)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (12.1.3.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (11.0.2.54)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (10.3.2.106)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (11.4.5.107)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (12.1.0.106)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (12.1.105)\n",
            "Requirement already satisfied: triton==3.1.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (3.1.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (1.13.1)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.11/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch>=1.11.0->sentence-transformers) (12.6.85)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=1.11.0->sentence-transformers) (1.3.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.4.1)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn->sentence-transformers) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn->sentence-transformers) (3.5.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=1.11.0->sentence-transformers) (3.0.2)\n",
            "Downloading fastapi-0.115.7-py3-none-any.whl (94 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m94.8/94.8 kB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading uvicorn-0.34.0-py3-none-any.whl (62 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.3/62.3 kB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading python_magic-0.4.27-py2.py3-none-any.whl (13 kB)\n",
            "Downloading pypdf2-3.0.1-py3-none-any.whl (232 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m232.6/232.6 kB\u001b[0m \u001b[31m9.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pinecone_client-5.0.1-py3-none-any.whl (244 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m244.8/244.8 kB\u001b[0m \u001b[31m17.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading python_dotenv-1.0.1-py3-none-any.whl (19 kB)\n",
            "Downloading groq-0.15.0-py3-none-any.whl (109 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m109.6/109.6 kB\u001b[0m \u001b[31m8.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pinecone_plugin_inference-1.1.0-py3-none-any.whl (85 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m85.4/85.4 kB\u001b[0m \u001b[31m7.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pinecone_plugin_interface-0.0.7-py3-none-any.whl (6.2 kB)\n",
            "Downloading starlette-0.45.2-py3-none-any.whl (71 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m71.5/71.5 kB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: uvicorn, python-magic, python-dotenv, PyPDF2, pinecone-plugin-interface, starlette, pinecone-plugin-inference, pinecone-client, groq, fastapi\n",
            "Successfully installed PyPDF2-3.0.1 fastapi-0.115.7 groq-0.15.0 pinecone-client-5.0.1 pinecone-plugin-inference-1.1.0 pinecone-plugin-interface-0.0.7 python-dotenv-1.0.1 python-magic-0.4.27 starlette-0.45.2 uvicorn-0.34.0\n"
          ]
        }
      ],
      "source": [
        "!pip install transformers fastapi uvicorn python-magic PyPDF2 sentence-transformers pinecone-client python-dotenv groq"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "punuLP9G17BN",
        "outputId": "c732c5bd-fbf5-453c-d577-ff1ff46b9fe6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting tiktoken\n",
            "  Downloading tiktoken-0.8.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.6 kB)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.11/dist-packages (4.25.5)\n",
            "Collecting python-multipart\n",
            "  Downloading python_multipart-0.0.20-py3-none-any.whl.metadata (1.8 kB)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.11/dist-packages (from tiktoken) (2024.11.6)\n",
            "Requirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.11/dist-packages (from tiktoken) (2.32.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.26.0->tiktoken) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.26.0->tiktoken) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.26.0->tiktoken) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.26.0->tiktoken) (2024.12.14)\n",
            "Downloading tiktoken-0.8.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m19.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading python_multipart-0.0.20-py3-none-any.whl (24 kB)\n",
            "Installing collected packages: python-multipart, tiktoken\n",
            "Successfully installed python-multipart-0.0.20 tiktoken-0.8.0\n"
          ]
        }
      ],
      "source": [
        "!pip install tiktoken protobuf python-multipart"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tGKQVLHf2AQX",
        "outputId": "4a0cacf5-bd34-415f-aa3a-c582266f3a6e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.47.1)\n",
            "Collecting transformers\n",
            "  Downloading transformers-4.48.1-py3-none-any.whl.metadata (44 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.4/44.4 kB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: sentencepiece in /usr/local/lib/python3.11/dist-packages (0.2.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers) (3.16.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.24.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.27.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (1.26.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.21.0)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.5.2)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers) (4.67.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.24.0->transformers) (2024.10.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.24.0->transformers) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2024.12.14)\n",
            "Downloading transformers-4.48.1-py3-none-any.whl (9.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.7/9.7 MB\u001b[0m \u001b[31m30.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: transformers\n",
            "  Attempting uninstall: transformers\n",
            "    Found existing installation: transformers 4.47.1\n",
            "    Uninstalling transformers-4.47.1:\n",
            "      Successfully uninstalled transformers-4.47.1\n",
            "Successfully installed transformers-4.48.1\n"
          ]
        }
      ],
      "source": [
        "!pip install --upgrade transformers sentencepiece"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c1QzlOlp2D76",
        "outputId": "2e6db322-d322-4753-c05d-7774302d0920"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pyngrok\n",
            "  Downloading pyngrok-7.2.3-py3-none-any.whl.metadata (8.7 kB)\n",
            "Requirement already satisfied: nest_asyncio in /usr/local/lib/python3.11/dist-packages (1.6.0)\n",
            "Requirement already satisfied: PyYAML>=5.1 in /usr/local/lib/python3.11/dist-packages (from pyngrok) (6.0.2)\n",
            "Downloading pyngrok-7.2.3-py3-none-any.whl (23 kB)\n",
            "Installing collected packages: pyngrok\n",
            "Successfully installed pyngrok-7.2.3\n"
          ]
        }
      ],
      "source": [
        "!pip install pyngrok nest_asyncio"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0Z3lr_-f2p2r",
        "outputId": "6c887f81-e416-481e-817f-c4c482f2e977"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Authtoken saved to configuration file: /root/.config/ngrok/ngrok.yml\n"
          ]
        }
      ],
      "source": [
        "!ngrok config add-authtoken 2s3QXaCH8tOJVBsWyL4cAM2oES1_2Kqi7tBNicc2go8En51tX"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "6TyVuEIV3-qO"
      },
      "outputs": [],
      "source": [
        "PINECONE_API_KEY=\"pcsk_34eGUu_HEdpsuceLRghrAoEdG4yE65d2McjuuS4wC4gQYtXP4JYZgBv8oVYD1m5CtqTkmL\"\n",
        "PINECONE_ENV='aws'\n",
        "GROQ_API_KEY=\"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\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "lp80xbSCRJC5"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "import traceback\n",
        "from typing import List, Dict\n",
        "from groq import Groq\n",
        "from sentence_transformers import SentenceTransformer\n",
        "from pinecone import Pinecone\n",
        "\n",
        "\n",
        "class RAGPipeline:\n",
        "    def __init__(\n",
        "        self,\n",
        "        embedding_model_name=\"all-MiniLM-L6-v2\",\n",
        "        pinecone_api_key=None,\n",
        "        pinecone_env=None,\n",
        "        groq_api_key=None,\n",
        "    ):\n",
        "        # Initialize embedding model\n",
        "        self.embedding_model = SentenceTransformer(embedding_model_name)\n",
        "\n",
        "        # Initialize Pinecone\n",
        "        self.pc = Pinecone(api_key=pinecone_api_key, environment=pinecone_env)\n",
        "        self.index = self.pc.Index(\"legal-llm\")\n",
        "\n",
        "        # Initialize Groq client\n",
        "        self.client = Groq(api_key=groq_api_key)\n",
        "\n",
        "    def query_pinecone(self, prompt: str, top_k: int = 10) -> List[Dict]:\n",
        "        \"\"\"Vector search with metadata filtering and scoring\"\"\"\n",
        "        query_embedding = self.embedding_model.encode(prompt).tolist()\n",
        "\n",
        "        query_results = self.index.query(\n",
        "            vector=query_embedding,\n",
        "            top_k=top_k,\n",
        "            include_metadata=True,\n",
        "            filter={\n",
        "                # Optional: Add metadata filtering logic\n",
        "                # \"domain\": \"legal\"\n",
        "            },\n",
        "        )\n",
        "\n",
        "        results = [\n",
        "            {\n",
        "                \"text\": match.metadata[\"text\"],\n",
        "                \"score\": match.score,\n",
        "                \"metadata\": match.metadata,\n",
        "            }\n",
        "            for match in query_results.matches\n",
        "        ]\n",
        "\n",
        "        return sorted(results, key=lambda x: x[\"score\"], reverse=True)\n",
        "\n",
        "    def get_context(self, user_prompt: str, top_k: int = 3) -> str:\n",
        "        \"\"\"Retrieve and process top contextual results\"\"\"\n",
        "        vector_results = self.query_pinecone(user_prompt, top_k)\n",
        "\n",
        "        return \"\\n\\n\".join([result[\"text\"] for result in vector_results])\n",
        "\n",
        "    def generate_response(self, user_query: str, context: str) -> Dict:\n",
        "        \"\"\"Response generation with structured output\"\"\"\n",
        "        system_prompt = f\"\"\"\n",
        "        You are an expert legal analyst. Provide precise, evidence-based responses.\n",
        "\n",
        "        Context: {context}\n",
        "\n",
        "        Response Guidelines:\n",
        "        - Analyze the query using ONLY the provided context\n",
        "        - Structure response as JSON with:\n",
        "          1. \"answer\": Comprehensive legal explanation\n",
        "          2. \"reasoning\": Logical breakdown\n",
        "          3. \"confidence_score\": 0-1 rating\n",
        "          4. \"key_sources\": Relevant context snippets\n",
        "        - Be concise but thorough\n",
        "        - Explicitly state if context is insufficient\n",
        "        \"\"\"\n",
        "\n",
        "        try:\n",
        "            response = self.client.chat.completions.create(\n",
        "                model=\"llama3-70b-8192\",\n",
        "                response_format={\"type\": \"json_object\"},\n",
        "                messages=[\n",
        "                    {\"role\": \"system\", \"content\": system_prompt},\n",
        "                    {\"role\": \"user\", \"content\": user_query},\n",
        "                ],\n",
        "                max_tokens=2048,\n",
        "                temperature=0.3,\n",
        "            )\n",
        "\n",
        "            return json.loads(response.choices[0].message.content)\n",
        "\n",
        "        except Exception as e:\n",
        "            return {\"error\": str(e), \"trace\": traceback.format_exc()}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 327,
          "referenced_widgets": [
            "56ec208c58b64fc2a91f356f47e7fd5d",
            "0b6bbcaf783147dabfd6e7b942e74b1c",
            "71a999c8cb124a62b1040d0a2feca051",
            "ca36f6b190734d7a985f9518a397fc89",
            "5b282df8f6ba466e8058d654ec2a8b02",
            "171fd30acb1a40438a83c66eb2ed9bca",
            "607d9809f18c47ddb633471de9aa8db3",
            "84d80b55fe134659a3b8ac21db3537aa",
            "806cb97398cc47caaf92b8e550abc2f6",
            "0b3f1485b5ed42d19d142c348012db37",
            "975ec76bc0904ebab7544461039286da"
          ]
        },
        "id": "gojDlzOpPLWD",
        "outputId": "215e8a4a-30d2-403e-bc29-025ab9f7fe13"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Public URL: https://2abf-34-82-22-217.ngrok-free.app\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:     Started server process [2519]\n",
            "INFO:     Waiting for application startup.\n",
            "INFO:     Application startup complete.\n",
            "INFO:     Uvicorn running on http://0.0.0.0:8000 (Press CTRL+C to quit)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Models and tokenizer loaded.\n",
            "INFO:     103.158.138.147:0 - \"GET / HTTP/1.1\" 200 OK\n",
            "INFO:     103.158.138.147:0 - \"GET /favicon.ico HTTP/1.1\" 404 Not Found\n",
            "INFO:     103.158.138.147:0 - \"POST /translate-pdf/ HTTP/1.1\" 200 OK\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Batches:   0%|          | 0/9 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "56ec208c58b64fc2a91f356f47e7fd5d"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Translated chunks embedded and stored in Pinecone.\n",
            "INFO:     103.158.138.147:0 - \"POST /query/ HTTP/1.1\" 200 OK\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:     Shutting down\n",
            "INFO:     Waiting for application shutdown.\n",
            "INFO:     Application shutdown complete.\n",
            "INFO:     Finished server process [2519]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Models and tokenizer resources released.\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import torch\n",
        "import uvicorn\n",
        "import asyncio\n",
        "import nest_asyncio\n",
        "import magic\n",
        "\n",
        "from torch.amp import autocast\n",
        "from fastapi import FastAPI, UploadFile, HTTPException, BackgroundTasks\n",
        "from transformers import MBartForConditionalGeneration, MBart50TokenizerFast\n",
        "from sentence_transformers import SentenceTransformer\n",
        "from pinecone import Pinecone, ServerlessSpec\n",
        "from io import BytesIO\n",
        "from PyPDF2 import PdfReader\n",
        "from pydantic import BaseModel\n",
        "\n",
        "from contextlib import asynccontextmanager\n",
        "from pydantic import Field\n",
        "from time import time\n",
        "from pyngrok import ngrok\n",
        "from concurrent.futures import ThreadPoolExecutor\n",
        "from uvicorn import Config, Server\n",
        "\n",
        "nest_asyncio.apply()\n",
        "\n",
        "# Initialize FastAPI app\n",
        "app = FastAPI()\n",
        "\n",
        "# Translation model and resources\n",
        "translation_model = \"facebook/mbart-large-50-many-to-one-mmt\"\n",
        "model = None\n",
        "tokenizer = None\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "# Initialize RAG pipeline\n",
        "pinecone_api_key = PINECONE_API_KEY\n",
        "pinecone_env = PINECONE_ENV\n",
        "groq_api_key = GROQ_API_KEY\n",
        "\n",
        "rag_pipeline = RAGPipeline(\n",
        "    embedding_model_name=\"all-MiniLM-L6-v2\",\n",
        "    pinecone_api_key=pinecone_api_key,\n",
        "    pinecone_env=pinecone_env,\n",
        "    groq_api_key=groq_api_key,\n",
        ")\n",
        "\n",
        "# ThreadPoolExecutor for parallel tasks\n",
        "executor = ThreadPoolExecutor(max_workers=4)\n",
        "\n",
        "# Pydantic model for query request\n",
        "class QueryRequest(BaseModel):\n",
        "    user_query: str = Field(\n",
        "        ..., min_length=5, title=\"User Query\", description=\"The legal question to be analyzed\"\n",
        "    )\n",
        "\n",
        "# Asynchronous context manager to load and release model resources\n",
        "@asynccontextmanager\n",
        "async def lifespan(app: FastAPI):\n",
        "    global model, tokenizer, embedding_model\n",
        "    model = MBartForConditionalGeneration.from_pretrained(translation_model).to(device)\n",
        "    tokenizer = MBart50TokenizerFast.from_pretrained(translation_model)\n",
        "    embedding_model = SentenceTransformer(\"sentence-transformers/all-MiniLM-L6-v2\").to(device)\n",
        "    print(\"Models and tokenizer loaded.\")\n",
        "    yield\n",
        "    del model, tokenizer, embedding_model\n",
        "    torch.cuda.empty_cache()\n",
        "    print(\"Models and tokenizer resources released.\")\n",
        "\n",
        "app.router.lifespan_context = lifespan\n",
        "\n",
        "# Function to detect MIME type\n",
        "def get_mime_type(file: UploadFile):\n",
        "    mime = magic.Magic(mime=True)\n",
        "    file_content = file.file.read(2048)\n",
        "    mime_type = mime.from_buffer(file_content)\n",
        "    file.file.seek(0)\n",
        "    return mime_type\n",
        "\n",
        "# Function to read PDF content\n",
        "def read_pdf(file: UploadFile):\n",
        "    pdf_content = BytesIO(file.file.read())\n",
        "    reader = PdfReader(pdf_content)\n",
        "    return \"\\n\".join(page.extract_text() for page in reader.pages if page.extract_text()).strip()\n",
        "\n",
        "# Function to split content into chunks\n",
        "def create_chunks(text: str, chunk_size: int, overlap: int):\n",
        "    words = text.split()\n",
        "    chunks = []\n",
        "    current_chunk = []\n",
        "    current_length = 0\n",
        "\n",
        "    for word in words:\n",
        "        word_length = len(word) + 1\n",
        "        if current_length + word_length > chunk_size:\n",
        "            chunks.append(\" \".join(current_chunk))\n",
        "            overlap_words = (\n",
        "                current_chunk[-overlap:]\n",
        "                if overlap <= len(current_chunk)\n",
        "                else current_chunk\n",
        "            )\n",
        "            current_chunk = overlap_words\n",
        "            current_length = sum(len(w) + 1 for w in current_chunk)\n",
        "\n",
        "        current_chunk.append(word)\n",
        "        current_length += word_length\n",
        "\n",
        "    if current_chunk:\n",
        "        chunks.append(\" \".join(current_chunk))\n",
        "\n",
        "    return chunks\n",
        "\n",
        "# Asynchronous function to translate chunks\n",
        "async def translate_chunks(chunks, batch_size):\n",
        "    translations = []\n",
        "\n",
        "    async def translate_batch(batch):\n",
        "        encoded_input = tokenizer(\n",
        "            batch, return_tensors=\"pt\", padding=True, truncation=True, max_length=1024\n",
        "        ).to(device)\n",
        "\n",
        "        with autocast(device_type=\"cuda\" if torch.cuda.is_available() else \"cpu\"):\n",
        "            generated_tokens = model.generate(\n",
        "                encoded_input[\"input_ids\"], max_length=128, num_beams=1\n",
        "            )\n",
        "\n",
        "        return tokenizer.batch_decode(generated_tokens, skip_special_tokens=True)\n",
        "\n",
        "    tasks = [\n",
        "        translate_batch(chunks[i: i + batch_size])\n",
        "        for i in range(0, len(chunks), batch_size)\n",
        "    ]\n",
        "    results = await asyncio.gather(*tasks)\n",
        "    for result in results:\n",
        "        translations.extend(result)\n",
        "\n",
        "    return translations\n",
        "\n",
        "# Function to store embeddings in Pinecone\n",
        "def store_embeddings(translated_chunks):\n",
        "    try:\n",
        "        if not pinecone_api_key:\n",
        "            raise ValueError(\n",
        "                \"PINECONE_API_KEY not found in environment variables. Ensure it is set in the .env file.\"\n",
        "            )\n",
        "\n",
        "        pc = Pinecone(api_key=pinecone_api_key)\n",
        "        index_name = \"legal-llm\"\n",
        "\n",
        "        if index_name in pc.list_indexes().names():\n",
        "            pc.delete_index(index_name)\n",
        "\n",
        "        pc.create_index(\n",
        "            name=index_name,\n",
        "            dimension=384,\n",
        "            metric=\"cosine\",\n",
        "            spec=ServerlessSpec(cloud=\"aws\", region=\"us-east-1\"),\n",
        "        )\n",
        "\n",
        "        index = pc.Index(index_name)\n",
        "        embeddings = embedding_model.encode(\n",
        "            translated_chunks, convert_to_tensor=True, show_progress_bar=True\n",
        "        ).cpu().numpy()\n",
        "\n",
        "        batch_size = 256  # Increased batch size\n",
        "        for i in range(0, len(translated_chunks), batch_size):\n",
        "            batch = [\n",
        "                (f\"doc_{j}\", embeddings[j], {\"text\": translated_chunks[j]})\n",
        "                for j in range(i, min(i + batch_size, len(translated_chunks)))\n",
        "            ]\n",
        "            index.upsert(vectors=batch)\n",
        "\n",
        "        print(\"Translated chunks embedded and stored in Pinecone.\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error storing embeddings in Pinecone: {str(e)}\")\n",
        "\n",
        "    return index_name\n",
        "\n",
        "# API Endpoints\n",
        "@app.get(\"/\")\n",
        "def read_root():\n",
        "    return {\"message\": \"Welcome to the LegalDoc-Translate-Query-Assistant portal!\"}\n",
        "\n",
        "@app.post(\"/translate-pdf/\")\n",
        "async def process_pdf(\n",
        "    file: UploadFile,\n",
        "    background_tasks: BackgroundTasks,\n",
        "    chunk_size: int = 500,\n",
        "    overlap: int = 50,\n",
        "    batch_size: int = 256,\n",
        "):\n",
        "    mime_type = get_mime_type(file)\n",
        "    if mime_type != \"application/pdf\":\n",
        "        raise HTTPException(status_code=400, detail=\"Invalid file type. Please upload a valid PDF file.\")\n",
        "\n",
        "    if file.size > 10 * 1024 * 1024:  # 10 MB limit\n",
        "        raise HTTPException(status_code=400, detail=\"File size exceeds 10 MB.\")\n",
        "\n",
        "    try:\n",
        "        start_time = time()\n",
        "        text = read_pdf(file)\n",
        "        chunks = create_chunks(text, chunk_size, overlap)\n",
        "        translated_chunks = await translate_chunks(chunks, batch_size)\n",
        "        processing_time = time() - start_time\n",
        "\n",
        "        # Run embedding storage in a separate thread\n",
        "        executor.submit(store_embeddings, translated_chunks)\n",
        "\n",
        "        return {\n",
        "            \"mime_type\": mime_type,\n",
        "            \"translated_chunks\": translated_chunks,\n",
        "            \"processing_time\": f\"{processing_time:.2f} seconds\",\n",
        "            \"pinecone_status\": \"Embedding storage initiated in the background.\",\n",
        "        }\n",
        "    except Exception as e:\n",
        "        raise HTTPException(status_code=500, detail=f\"Error processing the PDF: {str(e)}\")\n",
        "\n",
        "@app.post(\"/query/\")\n",
        "async def query_llm(request: QueryRequest):\n",
        "    try:\n",
        "        context = rag_pipeline.get_context(request.user_query)\n",
        "        if not context:\n",
        "            raise HTTPException(status_code=404, detail=\"No relevant context found for the query.\")\n",
        "\n",
        "        response = rag_pipeline.generate_response(request.user_query, context)\n",
        "        return response\n",
        "    except Exception as e:\n",
        "        raise HTTPException(status_code=500, detail=f\"Error processing query: {str(e)}\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    async def main():\n",
        "        try:\n",
        "            public_url = ngrok.connect(8000).public_url\n",
        "            print(f\"Public URL: {public_url}\")\n",
        "\n",
        "            config = uvicorn.Config(app, host=\"0.0.0.0\", port=8000, log_level=\"info\")\n",
        "            server = uvicorn.Server(config)\n",
        "\n",
        "            await server.serve()\n",
        "        except OSError as e:\n",
        "            if \"address already in use\" in str(e):\n",
        "                print(\"Port 8000 is already in use. Please stop the existing process or choose another port.\")\n",
        "            else:\n",
        "                raise e\n",
        "\n",
        "    try:\n",
        "        asyncio.run(main())\n",
        "    except RuntimeError as e:\n",
        "        if \"This event loop is already running\" in str(e):\n",
        "            print(\"Using an alternative execution due to running event loop.\")\n",
        "            task = asyncio.create_task(main())\n",
        "            await task\n",
        "    except KeyboardInterrupt:\n",
        "        print(\"Shutting down server.\")\n",
        "        ngrok.disconnect(public_url)\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "56ec208c58b64fc2a91f356f47e7fd5d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_0b6bbcaf783147dabfd6e7b942e74b1c",
              "IPY_MODEL_71a999c8cb124a62b1040d0a2feca051",
              "IPY_MODEL_ca36f6b190734d7a985f9518a397fc89"
            ],
            "layout": "IPY_MODEL_5b282df8f6ba466e8058d654ec2a8b02"
          }
        },
        "0b6bbcaf783147dabfd6e7b942e74b1c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_171fd30acb1a40438a83c66eb2ed9bca",
            "placeholder": "​",
            "style": "IPY_MODEL_607d9809f18c47ddb633471de9aa8db3",
            "value": "Batches: 100%"
          }
        },
        "71a999c8cb124a62b1040d0a2feca051": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_84d80b55fe134659a3b8ac21db3537aa",
            "max": 9,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_806cb97398cc47caaf92b8e550abc2f6",
            "value": 9
          }
        },
        "ca36f6b190734d7a985f9518a397fc89": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_0b3f1485b5ed42d19d142c348012db37",
            "placeholder": "​",
            "style": "IPY_MODEL_975ec76bc0904ebab7544461039286da",
            "value": " 9/9 [00:00&lt;00:00, 16.74it/s]"
          }
        },
        "5b282df8f6ba466e8058d654ec2a8b02": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "171fd30acb1a40438a83c66eb2ed9bca": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "607d9809f18c47ddb633471de9aa8db3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "84d80b55fe134659a3b8ac21db3537aa": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "806cb97398cc47caaf92b8e550abc2f6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "0b3f1485b5ed42d19d142c348012db37": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "975ec76bc0904ebab7544461039286da": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}